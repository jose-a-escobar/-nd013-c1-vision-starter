{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset\n",
    "\n",
    "\n",
    "In this notebook, we will perform an EDA (Exploratory Data Analysis) on the processed Waymo dataset (data in the `processed` folder). In the first part, you will create a function to display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading unweighted datasets: ['/data/waymo/*.tfrecord']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/data/waymo/*.tfrecord']\n",
      "INFO:tensorflow:Number of filenames to read: 103\n",
      "WARNING:tensorflow:From /data/virtual_envs/sdc-c1-gpu-augment/lib/python3.7/site-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
      "WARNING:tensorflow:From /data/virtual_envs/sdc-c1-gpu-augment/lib/python3.7/site-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n"
     ]
    }
   ],
   "source": [
    "dataset = get_dataset(\"/data/waymo/*.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function to display an image and the bounding boxes\n",
    "\n",
    "Implement the `display_instances` function below. This function takes a batch as an input and display an image with its corresponding bounding boxes. The only requirement is that the classes should be color coded (eg, vehicles in red, pedestrians in blue, cyclist in green)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg') #Images cannot be displayed without this line\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def display_instances(batch):\n",
    "    \"\"\"\n",
    "    This function takes a batch from the dataset and display the image with \n",
    "    the associated bounding boxes.\n",
    "    \"\"\"\n",
    "    # ADD CODE HERE\n",
    "    #Color mapping of classes\n",
    "    colormap = {1:[1,0,0], 2:[0,1,0], 4:[0,0,1]}\n",
    "    \n",
    "    small_batch = batch.take(20)\n",
    "\n",
    "    for sample in small_batch:        \n",
    "        image = sample['image'].numpy()\n",
    "        print(sample['filename'])\n",
    "        \n",
    "        #get the size of the image\n",
    "        width, height, _ = tf.shape(sample['image']).numpy()\n",
    "        \n",
    "        #create figure and axes\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        #Get groundtruth boxes and classes\n",
    "        bboxes  = sample['groundtruth_boxes']\n",
    "        classes = sample['groundtruth_classes']\n",
    "        \n",
    "        #Create rectangular patches for each of the boxes\n",
    "        for cl, bb in zip(classes,bboxes):\n",
    "            #Get rectangle coordinates\n",
    "            y1, x1, y2, x2 = bb.numpy()\n",
    "            #Correct reactangle coordinates\n",
    "            # ** width and height are the current image size\n",
    "            # ** 1920 and 1080 are the orginal image size (*.tfrecord) \n",
    "            #    and 79 is the offset in the y-direction\n",
    "            y1 = y1 * height * (height/1080) - 79\n",
    "            y2 = y2 * height * (height/1080) - 79\n",
    "            x1 = x1 * width  * (width/1920)\n",
    "            x2 = x2 * width  * (width/1920)\n",
    "            #Create and add rectangles to the image\n",
    "            rec = Rectangle((x1,y1), x2 - x1, y2 - y1, facecolor='none',\n",
    "                           edgecolor=colormap[cl.numpy()])\n",
    "            ax.add_patch(rec)\n",
    "                \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display 10 images \n",
    "\n",
    "Using the dataset created in the second cell and the function you just coded, display 10 random images with the associated bounding boxes. You can use the methods `take` and `shuffle` on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'segment-12012663867578114640_820_000_840_000_with_camera_labels_34.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10226164909075980558_180_000_200_000_with_camera_labels_81.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-11236550977973464715_3620_000_3640_000_with_camera_labels_14.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10075870402459732738_1060_000_1080_000_with_camera_labels_38.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-1022527355599519580_4866_960_4886_960_with_camera_labels_92.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-11343624116265195592_5910_530_5930_530_with_camera_labels_9.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-11126313430116606120_1439_990_1459_990_with_camera_labels_5.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10975280749486260148_940_000_960_000_with_camera_labels_41.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10793018113277660068_2714_540_2734_540_with_camera_labels_7.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10241508783381919015_2889_360_2909_360_with_camera_labels_39.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10226164909075980558_180_000_200_000_with_camera_labels_25.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-1022527355599519580_4866_960_4886_960_with_camera_labels_45.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10588771936253546636_2300_000_2320_000_with_camera_labels_34.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-11847506886204460250_1640_000_1660_000_with_camera_labels_0.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10876852935525353526_1640_000_1660_000_with_camera_labels_24.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-11940460932056521663_1760_000_1780_000_with_camera_labels_33.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-11967272535264406807_580_000_600_000_with_camera_labels_104.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-11183906854663518829_2294_000_2314_000_with_camera_labels_30.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-10588771936253546636_2300_000_2320_000_with_camera_labels_30.tfrecord', shape=(), dtype=string)\n",
      "tf.Tensor(b'segment-1022527355599519580_4866_960_4886_960_with_camera_labels_29.tfrecord', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "## STUDENT SOLUTION HERE\n",
    "display_instances(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional EDA\n",
    "\n",
    "In this last part, you are free to perform any additional analysis of the dataset. What else would like to know about the data?\n",
    "For example, think about data distribution. So far, you have only looked at a single file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg') #Images cannot be displayed without this line\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def display_instances_mosaic(batch):\n",
    "    \"\"\"\n",
    "    This function takes a batch from the dataset and display several images with \n",
    "    the associated bounding boxes.\n",
    "    \"\"\"\n",
    "    # ADD CODE HERE\n",
    "    #Color mapping of classes\n",
    "    colormap = {1:[1,0,0], 2:[0,1,0], 4:[0,0,1]}\n",
    "    \n",
    "    small_batch = batch.take(20)\n",
    "    #create figure and axes\n",
    "    #Figure will display 20 images\n",
    "    fig, ax = plt.subplots(4, 5, figsize=(20,10))\n",
    "    #Image counter\n",
    "    i_image = 0\n",
    "    for sample in small_batch:        \n",
    "        image = sample['image'].numpy()        \n",
    "        #get the size of the image\n",
    "        width, height, _ = tf.shape(sample['image']).numpy()\n",
    "        #Image index in the mosaic\n",
    "        x_image = i_image % 4\n",
    "        y_image = i_image % 5\n",
    "        ax[x_image, y_image].imshow(image)\n",
    "        \n",
    "        #Get groundtruth boxes and classes\n",
    "        bboxes  = sample['groundtruth_boxes']\n",
    "        classes = sample['groundtruth_classes']        \n",
    "        \n",
    "        #Create rectangular patches for each of the boxes\n",
    "        for cl, bb in zip(classes,bboxes):                                                \n",
    "            #Get rectangle coordinates\n",
    "            y1, x1, y2, x2 = bb.numpy()\n",
    "            #Correct reactangle coordinates\n",
    "            # ** width and height are the current image size\n",
    "            # ** 1920 and 1080 are the orginal image size (*.tfrecord) \n",
    "            #    and 79 is the offset in the y-direction\n",
    "            y1 = y1 * height * (height/1080) - 79\n",
    "            y2 = y2 * height * (height/1080) - 79\n",
    "            x1 = x1 * width  * (width/1920)\n",
    "            x2 = x2 * width  * (width/1920)\n",
    "            #Create and add rectangles to the image\n",
    "            rec = Rectangle((x1,y1), x2 - x1, y2 - y1, facecolor='none',\n",
    "                           edgecolor=colormap[cl.numpy()])\n",
    "            ax[x_image, y_image].add_patch(rec)\n",
    "            \n",
    "        ax[x_image, y_image].axis('off')\n",
    "        #Update image counter\n",
    "        i_image += 1\n",
    "        \n",
    "    plt.tight_layout()            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_instances_mosaic(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
